{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from models import CNN_Encoder, RNN_Decoder, image_features_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 768 # should not be equal units\n",
    "units = 512\n",
    "vocab_size = 14500 + 1\n",
    "CHECKPOINT_FOLDER = \"./checkpoint_dis/augmented_attention\"\n",
    "TOKENIZER_FOLDER = './tokenizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = CHECKPOINT_FOLDER\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./checkpoint_dis/augmented_attention/ckpt-6',\n",
       " './checkpoint_dis/augmented_attention/ckpt-7',\n",
       " './checkpoint_dis/augmented_attention/ckpt-8',\n",
       " './checkpoint_dis/augmented_attention/ckpt-9',\n",
       " './checkpoint_dis/augmented_attention/ckpt-10']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_manager.checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHOSE CHECKPOINT TO LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from ./checkpoint_dis/augmented_attention/ckpt-7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd0b411ad30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load some checkpoint\n",
    "\n",
    "n = 4\n",
    "\n",
    "checkpoint = ckpt_manager.checkpoints[-n]\n",
    "\n",
    "start_epoch = int(checkpoint.split('-')[-1])\n",
    "print(f'load from {checkpoint}')\n",
    "# restoring the latest checkpoint in checkpoint_path\n",
    "ckpt.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TOKENIZER_FOLDER,'tokenizer.json')) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('clean_cpations_n_files.csv')\n",
    "disney_captions = data.caption.to_list()\n",
    "disney_images= data.folder.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(disney_images,\n",
    "                                                                    disney_captions,\n",
    "                                                                    test_size=0.1,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img #, image_path\n",
    "\n",
    "\n",
    "def evaluate_temp(image, t):\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image), 0)\n",
    "    img_tensor_val_one, img_tensor_val_two = image_features_extract_model(temp_input)\n",
    "    img_tensor_val_one = tf.reshape(img_tensor_val_one, (img_tensor_val_one.shape[0], -1, img_tensor_val_one.shape[3]))\n",
    "    img_tensor_val_two = tf.reshape(img_tensor_val_two, (img_tensor_val_two.shape[0], -1, img_tensor_val_two.shape[3]))\n",
    "    features_one, features_two = encoder(img_tensor_val_one,img_tensor_val_two)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "    score = 0\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features_one, features_two, hidden)\n",
    "        \n",
    "        ### Slighlty randomize prediction - can be changed to Beam Search ###\n",
    "        \n",
    "        # SET TO 2 - 4 IF ARGMAX POlICY REQUIRED #\n",
    "        if i%2==0:\n",
    "            #predictions = tf.nn.softmax(predictions/t)\n",
    "            predicted_id = tf.random.categorical(predictions/t, 1)[0][0].numpy()\n",
    "        else:\n",
    "            predicted_id = np.argmax(predictions[0].numpy())\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, score/len(result)\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        score+=predictions[0][predicted_id].numpy()\n",
    "    \n",
    "    return result, score/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_path = [p.replace('features','disney_old/disney_img') for  p  in img_name_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### captions on the validation set\n",
    "\n",
    "#img_name_val, cap_val\n",
    "max_length = 27\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = cap_val[rid][1:]\n",
    "print ('Real Caption:', real_caption)\n",
    "\n",
    "for t in [0.5,0.7,0.9,1,1.2,1.5]:\n",
    "    res, a = evaluate_temp(image, t)\n",
    "    print(f\"{t}, {' '.join(res)}, score {a}\")\n",
    "Image.open(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
