{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from models import CNN_Encoder, RNN_Decoder, image_features_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('clean_cpations_n_files.csv')\n",
    "disney_captions = data.caption.to_list()\n",
    "disney_images= data.folder.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(t.split(\" \")) for t in disney_captions],bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vocab size: 14938\n",
      " captions size: 25488\n",
      " min len for captions 4\n",
      " max len for captions 14\n"
     ]
    }
   ],
   "source": [
    "words = [w.lower() for s in disney_captions for w in s.split(' ')]\n",
    "print(f' vocab size: {len(set(words))}')\n",
    "print(f' captions size: {len(disney_captions) }')\n",
    "print(f' min len for captions {min(len(t.split(\" \")) for t in disney_captions)}')\n",
    "print(f' max len for captions {max(len(t.split(\" \")) for t in disney_captions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "TOP_K = 14500 # vocab size\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000  # for shuffle \n",
    "embedding_dim = 768 # should not be equal units\n",
    "units = 512 # gru param\n",
    "vocab_size = TOP_K + 1\n",
    "num_steps = len(disney_images) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "# features_shape = 2048\n",
    "# attention_features_shape = 64\n",
    "\n",
    "# features_shape_two = 768\n",
    "# attention_features_two_shape = 17*17\n",
    "\n",
    "CHECKPOINT_FOLDER = \"./checkpoint_dis/augmented_attention\"\n",
    "TOKENIZER_FOLDER = './tokenizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = CHECKPOINT_FOLDER\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                          optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions vector shape (25488, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22939, 22939, 2549, 2549)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=TOP_K,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(disney_captions)\n",
    "\n",
    "## DISNEY CAPTIONS \n",
    "train_seqs = tokenizer.texts_to_sequences(disney_captions)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(os.path.join(TOKENIZER_FOLDER,'tokenizer.json'), 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "    \n",
    "with open(os.path.join(TOKENIZER_FOLDER,'tokenizer.json')) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "max_sent_len = 16\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen=max_sent_len)\n",
    "print(f'captions vector shape {cap_vector.shape}')\n",
    "\n",
    "\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(disney_images,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.1,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_aug(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, 10)\n",
    "        if bool(np.random.randint(2)):\n",
    "            img = img +  tf.keras.backend.random_normal((299,299,3),0,np.random.randint(20))\n",
    "        if bool(np.random.randint(2)):\n",
    "            img = tf.image.random_contrast(img, 0, 2)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    #img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    img_tensor = load_image_aug(img_name)\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from ./checkpoint_dis/augmented_attention/ckpt-10\n"
     ]
    }
   ],
   "source": [
    "## LOAD LAST CHECK POINT\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    print(f'load from {ckpt_manager.latest_checkpoint}')\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_path, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        batch_features_one, batch_features_two  = image_features_extract_model(img_path)\n",
    "        # 768               2048\n",
    "        \n",
    "        # reshape ex. from  [32, 8, 8, 2048] to [32, 64, 2048]\n",
    "        batch_features_one = tf.reshape(batch_features_one,\n",
    "                                    (batch_features_one.shape[0], -1, batch_features_one.shape[3]))  \n",
    "        \n",
    "        batch_features_two = tf.reshape(batch_features_two,\n",
    "                                    (batch_features_two.shape[0], -1, batch_features_two.shape[3]))\n",
    "        \n",
    "        features_one, features_two = encoder(batch_features_one, batch_features_two)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features_one, features_two, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 0 Loss 0.1192\n",
      "Epoch 11 Batch 100 Loss 0.0864\n",
      "Epoch 11 Batch 200 Loss 0.0824\n",
      "Epoch 11 Batch 300 Loss 0.1079\n",
      "Epoch 11 Loss 0.094246\n",
      "Time taken for 1 epoch 107.7023651599884 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SAVE EACH K EPOCHS\n",
    "\n",
    "K = 5\n",
    "\n",
    "EPOCHS = 11\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % K == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
