{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import string\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "from models import CNN_Encoder, RNN_Decoder, image_features_extract_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>captions</th>\n",
       "      <th>paths</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31723</th>\n",
       "      <td>32705</td>\n",
       "      <td>&lt;start&gt; remember dora the explorer this is her...</td>\n",
       "      <td>./disney_img/43556.jpg</td>\n",
       "      <td>remember dora the explorer? this is her now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31724</th>\n",
       "      <td>32706</td>\n",
       "      <td>&lt;start&gt; how to create a comedy legend &lt;end&gt;</td>\n",
       "      <td>./disney_img/43557.jpg</td>\n",
       "      <td>how to create a comedy legend.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           captions  \\\n",
       "31723       32705  <start> remember dora the explorer this is her...   \n",
       "31724       32706        <start> how to create a comedy legend <end>   \n",
       "\n",
       "                        paths                                        title  \n",
       "31723  ./disney_img/43556.jpg  remember dora the explorer? this is her now  \n",
       "31724  ./disney_img/43557.jpg               how to create a comedy legend.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('titles_orig_n_paths.csv')\n",
    "data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sent_len'] = data['title'].apply(lambda r: len(r.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fe93c3eb898>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.hist('sent_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = data[data['sent_len']<20].title.to_list()\n",
    "disney_images = data[data['sent_len']<20].paths.to_list()\n",
    "\n",
    "with open('titles.txt', 'w') as f:\n",
    "    for t in titles:\n",
    "        f.write(t+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbpe_tokenizer = SentencePieceBPETokenizer()\n",
    "sbpe_tokenizer.train(files='titles.txt', vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbpe_tokenizer.add_special_tokens(['<start>','<end>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbpe_tokenizer.save('./sbpe_tokenizer','spbe_tokenizer.e')\n",
    "out = sbpe_tokenizer.encode_batch(['<end> '+t+' <start>'  for t in titles])\n",
    "sbpe_titles= [t.tokens for t in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<end>', '▁how', '▁to', '▁celebrate', '▁4', '/', '20', '▁', '<start>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbpe_titles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(t) for t in sbpe_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29371 29371\n"
     ]
    }
   ],
   "source": [
    "titles_filtered = []\n",
    "disney_images_filtered = []\n",
    "\n",
    "for i,t in enumerate(sbpe_titles):\n",
    "    if len(t)<=20:\n",
    "        titles_filtered.append(t)\n",
    "        disney_images_filtered.append(disney_images[i])\n",
    "        \n",
    "disney_images = disney_images_filtered\n",
    "sbpe_titles = titles_filtered\n",
    "\n",
    "print(len(titles_filtered),len(disney_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(t) for t in sbpe_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [w.lower() for s in disney_captions for w in s.split(' ')]\n",
    "# print(f' vocab size: {len(set(words))}')\n",
    "# print(f' captions size: {len(disney_captions) }')\n",
    "# print(f' min len for captions {min(len(t.split(\" \")) for t in disney_captions)}')\n",
    "# print(f' max len for captions {max(len(t.split(\" \")) for t in disney_captions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000  # for shuffle \n",
    "embedding_dim = 768 # should not be equal units\n",
    "embedding_words = 300\n",
    "units = 512 # gru param\n",
    "vocab_size = VOCAB_SIZE + 1\n",
    "num_steps = len(disney_images) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "# features_shape = 2048\n",
    "# attention_features_shape = 64\n",
    "\n",
    "# features_shape_two = 768\n",
    "# attention_features_two_shape = 17*17\n",
    "\n",
    "CHECKPOINT_FOLDER = \"./checkpoint_dis/augmented_attention_sbpe\"\n",
    "TOKENIZER_FOLDER = './tokenizer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_words, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = CHECKPOINT_FOLDER\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                          optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "captions vector shape (29371, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(26433, 26433, 2938, 2938)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters=None)\n",
    "tokenizer.fit_on_texts(sbpe_titles)\n",
    "\n",
    "## DISNEY CAPTIONS \n",
    "train_seqs = tokenizer.texts_to_sequences(sbpe_titles)\n",
    "\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(os.path.join(TOKENIZER_FOLDER,'tokenizer_sbpe.json'), 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "    \n",
    "with open(os.path.join(TOKENIZER_FOLDER,'tokenizer_sbpe.json')) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
    "\n",
    "# Pad each vector to the max_length of the captions\n",
    "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
    "\n",
    "max_sent_len = 32\n",
    "\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post', maxlen=max_sent_len)\n",
    "print(f'captions vector shape {cap_vector.shape}')\n",
    "\n",
    "\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(disney_images,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.1,\n",
    "                                                                    random_state=42)\n",
    "\n",
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_aug(image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (299, 299))\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, 10)\n",
    "        if bool(np.random.randint(2)):\n",
    "            img = img +  tf.keras.backend.random_normal((299,299,3),0,np.random.randint(20))\n",
    "        if bool(np.random.randint(2)):\n",
    "            img = tf.image.random_contrast(img, 0, 2)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    #img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    img_tensor = load_image_aug(img_name)\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD LAST CHECK POINT\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    print(f'load from {ckpt_manager.latest_checkpoint}')\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_path, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        ### CNN Encoder\n",
    "        batch_features_one, batch_features_two  = image_features_extract_model(img_path)\n",
    "        # 768               2048\n",
    "        \n",
    "        # reshape ex. from  [32, 8, 8, 2048] to [32, 64, 2048]\n",
    "        batch_features_one = tf.reshape(batch_features_one,\n",
    "                                    (batch_features_one.shape[0], -1, batch_features_one.shape[3]))  \n",
    "        \n",
    "        batch_features_two = tf.reshape(batch_features_two,\n",
    "                                    (batch_features_two.shape[0], -1, batch_features_two.shape[3]))\n",
    "        \n",
    "        features_one, features_two = encoder(batch_features_one, batch_features_two)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features_one, features_two, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.0906\n",
      "Epoch 1 Batch 100 Loss 2.0210\n",
      "Epoch 1 Batch 200 Loss 1.8233\n"
     ]
    }
   ],
   "source": [
    "# SAVE EACH K EPOCHS\n",
    "\n",
    "K = 5\n",
    "\n",
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % K == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
